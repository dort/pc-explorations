2025.08.27 5:05am

I'm just wanting to get as much recorded as to how I'm going about this.

I suspect I will initially do a lot of experimentation that won't get checked into the repo, but my hope is that I'll just note what I try and what the results are here.

At this point I've copied over only 5 files into original/ just to have a record of where I started:

ni@rogr:~/repos/pc-explorations/charlie/pcn_intro_Stenlund_jax_port$ ls -latr original
total 84240
-rw-r--r--. 1 ni ni     2735 Aug 25 20:25 README.md
-rw-r--r--. 1 ni ni     1066 Aug 25 20:25 LICENSE
-rw-r--r--. 1 ni ni   735706 Aug 25 20:25 pcn_intro.pdf
-rw-r--r--. 1 ni ni 14310640 Aug 25 20:25 pcn_model_statedict.pth
-rw-r--r--. 1 ni ni 71203883 Aug 26 15:52 pcn_cifar10_notebook.ipynb
drwxr-xr-x. 1 ni ni      156 Aug 27 05:01 .
drwxr-xr-x. 1 ni ni      146 Aug 27 05:08 ..
ni@rogr:~/repos/pc-explorations/charlie/pcn_intro_Stenlund_jax_port$ 

and I've also now put what I think is "necessary" into this current directory:

ni@rogr:~/repos/pc-explorations/charlie/pcn_intro_Stenlund_jax_port$ ls -latr
total 69564
-rw-r--r--. 1 ni ni     2790 Aug 27 04:51 README.md
drwxr-xr-x. 1 ni ni        0 Aug 27 04:54 data
drwxr-xr-x. 1 ni ni       74 Aug 27 04:59 ..
drwxr-xr-x. 1 ni ni      156 Aug 27 05:01 original
-rw-r--r--. 1 ni ni 71203883 Aug 27 05:02 pcn_cifar10_notebook.ipynb
-rw-r--r--. 1 ni ni      999 Aug 27 05:11 working_journal.txt
-rw-r--r--. 1 ni ni    19555 Aug 27 05:15 LICENSE
drwxr-xr-x. 1 ni ni      146 Aug 27 05:15 .
ni@rogr:~/repos/pc-explorations/charlie/pcn_intro_Stenlund_jax_port$ ls -latr data
total 0
drwxr-xr-x. 1 ni ni   0 Aug 27 04:54 .
drwxr-xr-x. 1 ni ni 146 Aug 27 05:15 ..
ni@rogr:~/repos/pc-explorations/charlie/pcn_intro_Stenlund_jax_port$

The new license file is bigger than the original because I've appended the GPLv2 to the MIT which Monadillo released it under.

5:17am I'll check in to github in this current state and then pull it down into a brand new environment and do a "test" to see if it still works, reporting back here my results (ideally, everything will be successfull with trying to run the current code in the pcn_cifar10_notebook.ipynb that is here in this directory once I get a new venv set up that includes all the dependencies) after trying that (but not keeping anything that results and just discarding it).


5:45am I just shared the following update in a MM post with Matthew Behrend but it seems worthwhile to log it here too:

The first problem I seem to have come up against (which is an interesting one?) is that in the new venv I created in which I pip installed jax with cuda12 support, then also all the dependencies for the original, i've suddenly found that doing a baseline test run, with my only changes to the notebook file being I now have added:

import jax 
import jax.numpy as jnp

to the initial codeblock of imports, but making no other changes at all, when running the blocks in vscode one by one i got no errors or problems at all until I got down to the model training block, at which point a warning was thrown and my device being used is no longer cuda but is just my cpu

here's the warning:

Using device: cpu
Starting PCN training...
Epoch 1 / 4
  0%|          | 0/100 [00:00<?, ?it/s]/home/ni/sandboxen/virt-environs/s-m-pcn-intro-jax-test1/lib64/python3.13/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  warnings.warn(warn_msg)
/home/ni/sandboxen/virt-environs/s-m-pcn-intro-jax-test1/lib64/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
100%|██████████| 100/100 [11:48<00:00,  7.09s/it]
Epoch 2 / 4 

</end paste from MatterMost DM>

5:47am Obviously this is going to make testing excruciatingly slow if I can't solve this problem (though I can live with that if I have to (much less than ideal though))

5:49am  For the sake of documenting the hell out of things, the venv in which I tried to install jax w/cuda12 is this one:

ni@rogr:~/sandboxen/virt-environs/s-m-pcn-intro-jax-test1$

Perhaps it's only possible for one loaded python framework to get access to the GPU?

Maybe the way to go then is to not try to use the cuda version of JAX immediately?

So let me try that.

To install the non-cuda jax in a new venv along with all the torch stuff.


6:07am

All set w/this new venv:

(s-m-pcn-intro-jax-test2) ni@rogr:~/sandboxen/virt-environs/s-m-pcn-intro-jax-test2$ 


Now it's just a matter of waiting for this slow run to complete (it's more than 3/4 of the way done).


6:25am

i stepped afk and it finished: 49m 0.8s

12 seconds to compute test accuracies:

Test Top-1 Accuracy: 99.94%
Test Top-3 Accuracy: 100.00%


# How to load saved weights back into an identical model structure:

# Step 1: Recreate the model with the same architecture
# model_reloaded = PredictiveCodingNetwork(dims=layer_dims, output_dim=output_dim)

# OPTION 1 - load the weights from the GitHub repo mentioned in the manuscript
# from torch.hub import load_state_dict_from_url
# url = 'https://github.com/Monadillo/pcn-intro/raw/374774c6c493d419f266ab4fc305228a0dedae0b/pcn_model_statedict.pth'
# state_dict = load_state_dict_from_url(url, map_location='cpu')
# model_reloaded.load_state_dict(state_dict)

# OPTION 2 - load weights from a file you have saved locally
# save_path = 'pcn_model_statedict.pth' # or similar
# state_dict = torch.load(save_path, map_location='cpu')
# model_reloaded.load_state_dict(state_dict)

# Step 2: Move model to GPU if available
# if torch.cuda.is_available():
#    model_reloaded.to('cuda')

# Step 3: Set to evaluation mode (if the intention is to do inference only)
# model_reloaded.eval()


6:29am

so interesting...

i hadn't noticed those final steps (prior I think I'd only plotted using that first code block after testing the results and not scrolled all the way down)


6:41am

very interesting...

My plan didn't work. Even without installing the jax cuda12 version, torch is choosing cpu instead of cuda device for training (i stopped the process as there's no need to go through 49 more minutes in a 2nd training)